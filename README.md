# E-commerce Data Pipeline

Designed and implemented a modern **ETL data pipeline** using the **medallion architecture** (bronze â†’ silver â†’ gold layers) to process the **Looker E-commerce BigQuery Dataset** (sourced as CSV files from Kaggle) using bottom-up approach (Dimensional Data Modelling). The pipeline transforms raw e-commerce  data into clean, analytics-ready layers to enable deep insights.


## âš™ï¸ Tech Stack

- **Google BigQuery** â€“ Data warehouse  
- **Apache Airflow** â€“ Orchestration & scheduling  
- **SQL (BigQuery Standard SQL)** â€“ Data transformations  
- **Metabase** â€“ Analytics & dashboards  
- **GitHub** â€“ Version control & documentation  
- **Architecture Pattern**: Medallion architecture (bronze â†’ silver â†’ gold)
 

## Business Questions Addressed

The pipeline is built to answer the following key business questions:

1. **How do customers behave on the website?**  
   - Navigation patterns and common session flows  
   - Most frequent events (views, clicks, add-to-cart, etc.)  
   - Influence of traffic sources on user actions and conversions

2. **What factors drive customers to make a purchase?**  
   - Behavioral signals that precede purchases  
   - Traffic sources with the highest conversion rates  
   - Event sequences correlated with buying behavior

3. **How are different products performing?**  
   - Top-selling products and underlying reasons  
   - Differences in buyer characteristics by product  
   - Impact of product attributes (price, category, etc.) on sales

## Source Data Schema

The pipeline processes the **Looker E-commerce BigQuery Dataset** (originally from Kaggle), which consists of the following seven CSV files. Below is the schema for each table, including column names, data types, and brief descriptions where relevant.

### 1. distribution_centers.csv
Distribution centers / warehouses

- `id` (INT) â€” Distribution center ID (primary key)
- `name` (STRING) â€” Name of the distribution center
- `latitude` (FLOAT) â€” Latitude coordinate
- `longitude` (FLOAT) â€” Longitude coordinate

### 2. events.csv
User website interactions (page views, add-to-cart, purchases, etc.)

- `id` (INT) â€” Event ID (primary key)
- `user_id` (INT) â€” References `users.id`
- `sequence_number` (INT) â€” Order of events within a session
- `session_id` (STRING) â€” Unique session identifier
- `created_at` (TIMESTAMP) â€” When the event occurred
- `ip_address` (STRING) â€” Visitor IP address
- `city` (STRING) â€” City of the visitor
- `state` (STRING) â€” State / region
- `postal_code` (STRING) â€” Postal / ZIP code
- `browser` (STRING) â€” Browser used
- `traffic_source` (STRING) â€” Source of traffic (Organic, Paid, Email, Direct, etc.)
- `uri` (STRING) â€” Page or resource accessed
- `event_type` (STRING) â€” Type of event (e.g., 'Page View', 'Add to Cart', 'Purchase')

### 3. inventory_items.csv
Physical inventory records (snapshot of items in stock)

- `id` (INT) â€” Inventory item ID (primary key)
- `product_id` (INT) â€” References `products.id`
- `created_at` (TIMESTAMP) â€” When inventory record was created
- `sold_at` (TIMESTAMP) â€” When the item was sold (NULL if still in stock)
- `cost` (DECIMAL) â€” Cost to the company
- `product_category` (STRING) â€” Product category
- `product_name` (STRING) â€” Product name
- `product_brand` (STRING) â€” Brand name
- `product_retail_price` (DECIMAL) â€” Retail price
- `product_department` (STRING) â€” Department (e.g., Women, Men)
- `product_sku` (STRING) â€” Stock Keeping Unit
- `product_distribution_center_id` (INT) â€” References `distribution_centers.id`

### 4. order_items.csv
Line items within orders (the most granular sales data)

- `id` (INT) â€” Order item ID (primary key)
- `order_id` (INT) â€” References `orders.order_id`
- `user_id` (INT) â€” References `users.id`
- `product_id` (INT) â€” References `products.id`
- `inventory_item_id` (INT) â€” References `inventory_items.id`
- `status` (STRING) â€” Order item status (Processing, Shipped, Delivered, Returned, Cancelled, Complete)
- `created_at` (TIMESTAMP) â€” When the order item was created
- `shipped_at` (TIMESTAMP) â€” When shipped
- `delivered_at` (TIMESTAMP) â€” When delivered
- `returned_at` (TIMESTAMP) â€” When returned (NULL if not returned)

### 5. orders.csv
Order headers

- `order_id` (INT) â€” Order ID (primary key)
- `user_id` (INT) â€” References `users.id`
- `status` (STRING) â€” Overall order status
- `gender` (STRING) â€” Gender of the purchaser (Women, Men)
- `created_at` (TIMESTAMP) â€” Order creation timestamp
- `returned_at` (TIMESTAMP) â€” When the order was returned (NULL if not returned)
- `shipped_at` (TIMESTAMP) â€” When the order was shipped
- `delivered_at` (TIMESTAMP) â€” When the order was delivered
- `num_of_item` (INT) â€” Number of items in the order

### 6. products.csv
Product catalog

- `id` (INT) â€” Product ID (primary key)
- `cost` (DECIMAL) â€” Cost to the company
- `category` (STRING) â€” Product category
- `name` (STRING) â€” Product name
- `brand` (STRING) â€” Brand name
- `retail_price` (DECIMAL) â€” Selling price
- `department` (STRING) â€” Department (Women, Men)
- `sku` (STRING) â€” Stock Keeping Unit
- `distribution_center_id` (INT) â€” References `distribution_centers.id`

### 7. users.csv
Registered customers

- `id` (INT) â€” User ID (primary key)
- `first_name` (STRING)
- `last_name` (STRING)
- `email` (STRING)
- `age` (INT)
- `gender` (STRING) â€” Gender
- `state` (STRING) â€” State / region
- `street_address` (STRING) â€” Street address
- `postal_code` (STRING) â€” Postal / ZIP code
- `city` (STRING) â€” City
- `country` (STRING) â€” Country
- `latitude` (FLOAT) â€” Latitude
- `longitude` (FLOAT) â€” Longitude
- `traffic_source` (STRING) â€” How the user originally arrived (Organic, Adwords, Email, etc.)
- `created_at` (TIMESTAMP) â€” Account creation timestamp

**Note**:  
Not all tables/columns are used in the core analytical model. The pipeline focuses on data relevant to customer behavior, purchase drivers, and product performance (see [Data Scope & Profiling](#data-scope--profiling) and [Dimensional Modeling](#dimensional-modeling---kimball-type-2)).

## Data Scope & Profiling

**Focused Scope**  
Only data relevant to the business questions above was included. Non-essential tables were excluded during profiling to reduce complexity and ensure alignment with core analytics needs.

**Tables Excluded**  
- `distribution_centers` â€” logistics-focused, not directly tied to customer behavior or purchases  
- `inventory_items` â€” physical stock tracking; product details are sufficiently covered by `products` and `order_items`

**Included Sources** (based on TheLook e-commerce schema)  
- Events (user interactions: views, clicks, add-to-cart, etc.)  
- Order Items (purchases, status, revenue)  
- Users (customer attributes)  
- Products (item details, categories, price)  

## Architecture Overview

![E-commerce Data Pipeline Architecture](docs/images/etl.png)
The pipeline follows the **medallion pattern**:

## ğŸ§± Data Layers Explained

### Extract System
   Data landed in GCS with partitioned structure:  
   gs://your-bucket/raw-data/ingestion_date=events/date=2026-02-07/file.csv

### ğŸ¥‰ Bronze Layer
- Raw ingestion from source systems  
- Minimal transformations  
- Partitioned by `ingestion_date`  
- Acts as a replayable source of truth  

---

### ğŸ¥ˆ Silver Layer
- Deduplication using window functions  
- Data type normalization  
- Data validation:
  - Invalid IDs  
  - Invalid timestamps  
  - Known / allowed event types  
- Business-cleaned fields 

---

### ğŸ¥‡ Gold Layer
- Star schema optimized for analytics  

#### Fact Tables
- `fact_orders`
- `fact_order_items`
- `fact_events`

#### Dimension Tables
- `dim_users`
- `dim_products`
- `dim_event_type`
- `dim_date`
- `dim_location`
- `dim_order_status`
- `dim_browser`

- Surrogate keys generated using deterministic hashes  
- Incremental loading using `MERGE`
- Support for slowly changing dimensions (Type 2 updates) 


## Bus Matrix 

### Step 1: Identified Business Processes

Based on core business questions, the following **business processes** are modeled:

1. **Website Interaction / User Events**  
   Tracking every user action on the site (views, clicks, add-to-cart, etc.)

2. **Website Sessions / Visits**  
   Aggregated session-level behavior and traffic attribution

3. **Product Purchases / Sales**  
   Granular item-level sales and revenue (order line items)

4. **Order Fulfillment Lifecycle**  
   Tracking order status progression (created â†’ shipped â†’ delivered â†’ returned)

### Step 2: Final Bus Matrix

| Business Process                     | Fact Table                     | Date / Time | User | Product | Event Type | Traffic Source | Session | Order Status | Page / URI | Browser | Location | Business Questions Supported |
|--------------------------------------|--------------------------------|-------------|----------|---------|------------|----------------|---------|--------------|------------|---------|----------|--------------------------------|
| Website Interaction / User Events    | Fact_Events                | âœ…          | âœ…       | âšª       | âœ…         | âœ…             | âœ…      | â€”            | âœ…         | âœ…      | âœ…       | Navigation patterns, event frequency, traffic influence, funnels |
| Website Sessions / Visits            | Fact_Sessions                  | âœ…          | âœ…       | â€”       | â€”          | âœ…             | âœ…      | â€”            | â€”          | âœ…      | âœ…       | Visit patterns, engagement, traffic attribution, bounce rates |
| Product Purchases / Sales            | Fact_Order_Items         | âœ…          | âœ…       | âœ…      | â€”          | âšª             | â€”       | âœ…           | â€”          | â€”       | âœ…       | Revenue, units sold, top products, buyer profiles, conversion drivers |
| Order Fulfillment Lifecycle          | Fact_Orders                    | âœ…          | âœ…       | â€”       | â€”          | â€”              | â€”       | âœ…           | â€”          | â€”       | â€”        | Fulfillment KPIs, shipping times, return rates, status monitoring |

**Legend**  
âœ… = Core / Required dimension  
âšª = Optional / Contextual (can be included depending on analysis)  
â€” = Not applicable

### Conformed Dimensions (Shared Across Facts)

The following dimensions are **conformed** (designed once, reused everywhere):

- **Dim_Date** (and role-playing variants: order_date, shipped_date, delivered_date, returned_date)
- **Dim_Time**
- **Dim_Customer** (scd 2) 
- **Dim_Product** (scd2)
- **Dim_Traffic_Source** (especially important for attribution analysis)
- **Dim_Location** (city, state, postal_code â€” can be derived from events/users)

## Data Modeling

The data warehouse follows the **Kimball dimensional modeling** approach.  
This methodology is optimized for analytical queries and delivers clear, business-friendly results for stakeholders.

### Modeling Stages

#### Conceptual Model
The high-level business view of the system.  
It focuses on **core entities** and their **natural relationships**, without technical implementation details.

**Purpose**:  
Show what matters to the business â€” users, website events, sessions, purchases, products, etc.



![Conceptual Model](docs/images/conceptual.jpg)  


#### Logical Model
Adds more structure to the conceptual model while remaining technology-independent.

**Key elements defined**:
- Primary keys (business keys + surrogate keys)
- Attributes (columns) for each entity
- Relationships between fact tables and dimension tables
- Cardinality and optionality
- Grain of each fact table

Data types are suggested but not strictly enforced at this stage.
 

*Logical Model â€” Kimball star schema with facts and conformed dimensions*

#### Physical Model
The actual implementation in the target database (**BigQuery** in this project).

**Includes**:
- Final column names and data types
- Partitioning strategy (e.g. by date)
- Clustering keys for performance
- Storage format considerations
- Indexes / materialized views / query optimization decisions

![Physical Model](docs/images/ERD.jpg)  

*Physical Model â€” BigQuery tables with partitioning & clustering*

### Summary of Modeling Approach

| Stage          | Focus                              | Technical Detail | Target Audience          | Deliverable Example                  |
|----------------|------------------------------------|------------------|---------------------------|--------------------------------------|
| Conceptual     | Business entities & relationships  | None             | Business stakeholders     | High-level ER diagram                |
| Logical        | Facts, dimensions, keys, grain     | Low              | Data modelers & analysts  | Star schema diagram                  |
| Physical       | Database-specific implementation   | High             | Engineers & DBAs          | Table definitions + optimization     |


## Prerequisites

- **Docker Desktop** (recommended) â€” for local development and testing  
- **Git** â€” to clone the repository  
- **Python 3.x** â€” for any helper scripts (e.g., `generate_data.py`)  
- **Google Cloud Platform (GCP) Project** with billing enabled  

**GCP Setup**  
1. Create Cloud Storage buckets:  
- `gs://bronze-data-ecom` (raw CSVs)  


2. Enable required APIs:  
- BigQuery API  
- Cloud Storage API  
- IAM API, Service Account Credentials API  

3. Create a **Service Account** with:  
- Storage Object Admin (on all three buckets)  
- BigQuery Data Editor  
- BigQuery Job User  

4. Download the JSON key â†’ save as `gcp_credentials.json` in the project root

## Getting Started

```bash
# Clone the repo
git clone https://github.com/shalini-p16/ecommerce-data-pipeline.git
cd ecommerce-data-pipeline

# Run the pipeline (example â€” adapt to your tool: dbt, DLT, Airflow, etc.)
docker-compose up
```


```markdown
## Project Structure

```text
ECOMMERCE-DATA-PIPELINE/
â”œâ”€ config/
â”œâ”€ dags/
â”‚  â”œâ”€ __init__.py
â”‚  â””â”€ etl_dag.py
â”œâ”€ docs/
â”œâ”€ logs/
â”œâ”€ plugins/
â”œâ”€ sql/
â”‚  â”œâ”€ gold/
â”‚  â”‚  â”œâ”€ dim_browser.sql
â”‚  â”‚  â”œâ”€ dim_date.sql
â”‚  â”‚  â”œâ”€ dim_event_type.sql
â”‚  â”‚  â”œâ”€ dim_location.sql
â”‚  â”‚  â”œâ”€ dim_order_status.sql
â”‚  â”‚  â”œâ”€ dim_page.sql
â”‚  â”‚  â”œâ”€ dim_products.sql
â”‚  â”‚  â”œâ”€ dim_time.sql
â”‚  â”‚  â”œâ”€ dim_traffic_source.sql
â”‚  â”‚  â”œâ”€ dim_users.sql
â”‚  â”‚  â”œâ”€ fact_events.sql
â”‚  â”‚  â”œâ”€ fact_order_items.sql
â”‚  â”‚  â””â”€ fact_sessions.sql
â”‚  â””â”€ silver/
â”‚     â”œâ”€ distribution_centers.sql
â”‚     â”œâ”€ events.sql
â”‚     â”œâ”€ inventory_items.sql
â”‚     â”œâ”€ order_items.sql
â”‚     â”œâ”€ orders.sql
â”‚     â”œâ”€ products.sql
â”‚     â””â”€ users.sql
â””â”€ src/
   â”œâ”€ __init__.py
   â””â”€ extract.py

```

ğŸ› ï¸ Challenges

Mismatched data types across layers
â†’ Standardized user_id types in Silver layer
Schema Issue

### Future Enhancements

- Implement true **incremental CDC** (change data capture) instead of full daily loads
- Introduce **data lineage** and **observability** (e.g. via dbt docs, Monte Carlo, or similar tools)
- Add automated **data quality monitoring** and alerting
- Support **schema evolution** and backward-compatible transformations

---
