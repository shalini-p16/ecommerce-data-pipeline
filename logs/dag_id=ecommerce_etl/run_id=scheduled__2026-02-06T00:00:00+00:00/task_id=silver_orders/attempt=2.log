[2026-02-07T14:57:47.334+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: ecommerce_etl.silver_orders scheduled__2026-02-06T00:00:00+00:00 [queued]>
[2026-02-07T14:57:47.338+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: ecommerce_etl.silver_orders scheduled__2026-02-06T00:00:00+00:00 [queued]>
[2026-02-07T14:57:47.338+0000] {taskinstance.py:2170} INFO - Starting attempt 2 of 2
[2026-02-07T14:57:47.344+0000] {taskinstance.py:2191} INFO - Executing <Task(BigQueryInsertJobOperator): silver_orders> on 2026-02-06 00:00:00+00:00
[2026-02-07T14:57:47.347+0000] {standard_task_runner.py:60} INFO - Started process 1528 to run task
[2026-02-07T14:57:47.349+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'ecommerce_etl', 'silver_orders', 'scheduled__2026-02-06T00:00:00+00:00', '--job-id', '123', '--raw', '--subdir', 'DAGS_FOLDER/etl_dag.py', '--cfg-path', '/tmp/tmpjfcnhy43']
[2026-02-07T14:57:47.350+0000] {standard_task_runner.py:88} INFO - Job 123: Subtask silver_orders
[2026-02-07T14:57:47.375+0000] {task_command.py:423} INFO - Running <TaskInstance: ecommerce_etl.silver_orders scheduled__2026-02-06T00:00:00+00:00 [running]> on host 1d9eed85a9d4
[2026-02-07T14:57:47.409+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='ecommerce_etl' AIRFLOW_CTX_TASK_ID='silver_orders' AIRFLOW_CTX_EXECUTION_DATE='2026-02-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2026-02-06T00:00:00+00:00'
[2026-02-07T14:57:47.413+0000] {connection.py:234} WARNING - Connection schemes (type: google_cloud_platform) shall not contain '_' according to RFC3986.
[2026-02-07T14:57:47.414+0000] {base.py:83} INFO - Using connection ID 'google_cloud_default' for task execution.
[2026-02-07T14:57:47.414+0000] {credentials_provider.py:353} INFO - Getting connection using `google.auth.default()` since no explicit credentials are provided.
[2026-02-07T14:57:47.452+0000] {bigquery.py:2807} INFO - Executing: {'query': {'query': "WITH DeduplicatedOrders AS (\n SELECT\n   *,\n   ROW_NUMBER() OVER(PARTITION BY order_id, created_at ORDER BY created_at) as rn -- Assuming order_id and created_at uniquely identify an order\n FROM\n   `prism-486509.ecom_dataset.orders`\n),\nValidatedAndCleanedOrders AS (\n SELECT\n   -- Explicit Data Type Casting and Null Handling\n   order_id, -- Already INT64, included for completeness\n   COALESCE(SAFE_CAST(user_id AS INT64), 0) AS user_id, -- Casts to INT64, replaces NULLs with 0\n   COALESCE(status, 'UNKNOWN') AS status_cleaned, -- Replaces NULL status with 'UNKNOWN'\n   COALESCE(gender, 'UNKNOWN') AS gender_cleaned, -- Replaces NULL gender with 'UNKNOWN'\n   created_at, -- Already TIMESTAMP\n   returned_at, -- Can be NULL, no specific replacement needed unless specified\n   shipped_at, -- Can be NULL, no specific replacement needed unless specified\n   delivered_at, -- Can be NULL, no specific replacement needed unless specified\n   COALESCE(SAFE_CAST(num_of_item AS INT64), 0) AS num_of_item, -- Casts to INT64, replaces NULLs with 0\n\n\n   -- Basic Validation Checks\n   CASE WHEN order_id IS NULL OR order_id <= 0 THEN FALSE ELSE TRUE END AS is_valid_order_id,\n   CASE WHEN COALESCE(SAFE_CAST(user_id AS INT64), 0) IS NULL OR COALESCE(SAFE_CAST(user_id AS INT64), 0) <= 0 THEN FALSE ELSE TRUE END AS is_valid_user_id,\n   CASE WHEN created_at IS NULL OR created_at > CURRENT_TIMESTAMP() THEN FALSE ELSE TRUE END AS is_valid_created_at,\n   CASE\n     WHEN status IN ('Pending', 'Processing', 'Shipped', 'Delivered', 'Cancelled', 'Returned') THEN TRUE\n     ELSE FALSE\n   END AS is_known_status,\n   CASE\n     WHEN gender IN ('M', 'F', 'Other') THEN TRUE\n     ELSE FALSE\n   END AS is_known_gender,\n   CASE WHEN COALESCE(SAFE_CAST(num_of_item AS INT64), 0) < 0 THEN FALSE ELSE TRUE END AS is_valid_num_of_item,\n   -- Temporal consistency checks\n   CASE WHEN shipped_at IS NOT NULL AND shipped_at < created_at THEN FALSE ELSE TRUE END AS is_valid_shipped_time,\n   CASE WHEN delivered_at IS NOT NULL AND shipped_at IS NOT NULL AND delivered_at < shipped_at THEN FALSE ELSE TRUE END AS is_valid_delivered_time,\n   CASE WHEN returned_at IS NOT NULL AND delivered_at IS NOT NULL AND returned_at < delivered_at THEN FALSE ELSE TRUE END AS is_valid_returned_time\n\n\n FROM\n   DeduplicatedOrders\n WHERE rn = 1 -- Keep only the first occurrence for deduplication\n   AND order_id IS NOT NULL -- Exclude rows with a null 'order_id' as it's a critical identifier\n)\nSELECT\n order_id,\n user_id,\n status_cleaned AS status, -- Use the cleaned status\n gender_cleaned AS gender, -- Use the cleaned gender\n created_at,\n returned_at,\n shipped_at,\n delivered_at,\n num_of_item,\n is_valid_order_id,\n is_valid_user_id,\n is_valid_created_at,\n is_known_status,\n is_known_gender,\n is_valid_num_of_item,\n is_valid_shipped_time,\n is_valid_delivered_time,\n is_valid_returned_time\nFROM\n ValidatedAndCleanedOrders\nWHERE\n is_valid_order_id = TRUE\n AND is_valid_user_id = TRUE\n AND is_valid_created_at = TRUE\n AND is_known_status = TRUE\n AND is_known_gender = TRUE\n AND is_valid_num_of_item = TRUE\n AND is_valid_shipped_time = TRUE\n AND is_valid_delivered_time = TRUE\n AND is_valid_returned_time = TRUE; -- Filter to only include rows passing all basic validation", 'useLegacySql': False}}'
[2026-02-07T14:57:47.452+0000] {bigquery.py:1596} INFO - Inserting job ***_ecommerce_etl_silver_orders_2026_02_06T00_00_00_00_00_80f2c87c60a16288bfef2014f7a68128
[2026-02-07T14:57:48.285+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/google/cloud/operators/bigquery.py", line 2864, in execute
    job.result(timeout=self.result_timeout, retry=self.result_retry)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1595, in result
    do_get_result()
  File "/home/airflow/.local/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py", line 1584, in do_get_result
    super(QueryJob, self).result(retry=retry, timeout=timeout)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py", line 971, in result
    return super(_AsyncJob, self).result(timeout=timeout, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/google/api_core/future/polling.py", line 261, in result
    raise self._exception
google.api_core.exceptions.NotFound: 404 Not found: Table prism-486509:ecom_dataset.orders was not found in location europe-west1; reason: notFound, message: Not found: Table prism-486509:ecom_dataset.orders was not found in location europe-west1

Location: europe-west1
Job ID: airflow_ecommerce_etl_silver_orders_2026_02_06T00_00_00_00_00_80f2c87c60a16288bfef2014f7a68128

[2026-02-07T14:57:48.293+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=ecommerce_etl, task_id=silver_orders, execution_date=20260206T000000, start_date=20260207T145747, end_date=20260207T145748
[2026-02-07T14:57:48.303+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 123 for task silver_orders (404 Not found: Table prism-486509:ecom_dataset.orders was not found in location europe-west1; reason: notFound, message: Not found: Table prism-486509:ecom_dataset.orders was not found in location europe-west1

Location: europe-west1
Job ID: airflow_ecommerce_etl_silver_orders_2026_02_06T00_00_00_00_00_80f2c87c60a16288bfef2014f7a68128
; 1528)
[2026-02-07T14:57:48.353+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2026-02-07T14:57:48.365+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
